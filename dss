pra1:-Pre-processiang methods 
for text data (tokenization, stop-words etcâ€¦)
# Example text data
text_data = [
    "Natural Language Processing is amazing!",
    "Preprocessing includes tokenization, removing stop-words, and more.",
    "Text preprocessing prepares raw text for machine learning."
]
import nltk
from nltk.tokenize import word_tokenize

# Download necessary NLTK data
nltk.download('punkt')

# Tokenizing the text
tokens = [word_tokenize(sentence) for sentence in text_data]
print(tokens)
tokens_lowercase = [[word.lower() for word in token_list] for token_list in tokens]
print(tokens_lowercase)
import string

tokens_no_punctuation = [
    [word for word in token_list if word not in string.punctuation]
    for token_list in tokens_lowercase
]
print(tokens_no_punctuation)
from nltk.corpus import stopwords

# Download NLTK stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

tokens_no_stopwords = [
    [word for word in token_list if word not in stop_words]
    for token_list in tokens_no_punctuation
]
print(tokens_no_stopwords)
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()
tokens_stemmed = [
    [stemmer.stem(word) for word in token_list]
    for token_list in tokens_no_stopwords
]
print(tokens_stemmed)
from nltk.stem import WordNetLemmatizer

# Download WordNet data
nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()
tokens_lemmatized = [
    [lemmatizer.lemmatize(word) for word in token_list]
    for token_list in tokens_no_stopwords
]
print(tokens_lemmatized)
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
bow = vectorizer.fit_transform(text_data)
print(bow.toarray())
print(vectorizer.get_feature_names_out())
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer()
tfidf = tfidf_vectorizer.fit_transform(text_data)
print(tfidf.toarray())
print(tfidf_vectorizer.get_feature_names_out())


@@@@PRticla2:Implement Stemming

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import nltk

# Download necessary data
nltk.download('punkt')

# Sample text data
text_data = [
    "I am enjoying programming and learning Python.",
    "Programmers love solving problems and debugging codes.",
    "Stemming helps in reducing words to their base form."
]

# Initialize the PorterStemmer
stemmer = PorterStemmer()

# Tokenize and stem
for sentence in text_data:
    print(f"Original Sentence: {sentence}")
    tokens = word_tokenize(sentence)
    stemmed_tokens = [stemmer.stem(word) for word in tokens]
    print(f"Stemmed Tokens: {stemmed_tokens}\n")


!!!!pora3:Implement Morphological Analysis
import nltk
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag, word_tokenize

# Download necessary resources
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

# Example text
text = "The children are playing happily in the playground. She was running faster than anyone else."

# Tokenize and POS tagging
tokens = word_tokenize(text)
pos_tags = pos_tag(tokens)

# Perform lemmatization with part-of-speech
for word, pos in pos_tags:
    lemma = lemmatizer.lemmatize(word.lower())
    print(f"Word: {word}, POS: {pos}, Lemma: {lemma}")


&&&prat4:Implement N-gram Model

# Function to generate N-grams
def generate_ngrams(text, n):
    tokens = text.split()  # Tokenize the text
    ngrams = [tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]
    return ngrams

# Example text
text = "Natural language processing is amazing."

# Generate bi-grams (N = 2)
bigrams = generate_ngrams(text, 2)
print("Bi-grams:", bigrams)

# Generate tri-grams (N = 3)
trigrams = generate_ngrams(text, 3)
print("Tri-grams:", trigrams)


####pra5:Implement Part-of-Speech tagging
import nltk

# Download required NLTK data files
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Example text
text = "Natural Language Processing is an amazing field of study."

# Tokenize the text into words
tokens = nltk.word_tokenize(text)

# Perform POS tagging
pos_tags = nltk.pos_tag(tokens)

# Display the results
print("Word and POS Tag:")
for word, tag in pos_tags:
    print(f"{word}: {tag}")


&&&pta6:Implement Chucking

import nltk

# Download required NLTK data
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Example text
text = "The quick brown fox jumps over the lazy dog."

# Tokenize and tag the text
tokens = nltk.word_tokenize(text)
pos_tags = nltk.pos_tag(tokens)

# Define a chunk grammar
# This example captures noun phrases (NP) using POS tags (e.g., DT + JJ + NN)
chunk_grammar = r"""
  NP: {<DT>?<JJ>*<NN>}   # NP: Determiner (optional), Adjective(s), and Noun
"""

# Create a chunk parser
chunk_parser = nltk.RegexpParser(chunk_grammar)

# Parse the tagged text
chunk_tree = chunk_parser.parse(pos_tags)

# Display the chunk tree
print(chunk_tree)
chunk_tree.draw()  # Opens a graphical display of the tree (if supported)


$$$pra7:-mplement Text Summarization

# Install necessary library
# pip install transformers

from transformers import pipeline

# Load a pre-trained summarization pipeline
summarizer = pipeline("summarization")

# Example text
text = """
Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence (AI) 
that enables computers to understand and interpret human language. Applications of NLP include 
sentiment analysis, machine translation, chatbots, and more. By leveraging techniques like text summarization, 
NLP systems can distill vast amounts of information into concise summaries, making it easier for users to process and understand data.
"""

# Summarize the text
summary = summarizer(text, max_length=50, min_length=25, do_sample=False)
print("Abstractive Summary:")
print(summary[0]['summary_text'])


****prat8:-Implement Named Entity Recognition

import spacy

# Load the spaCy English language model
nlp = spacy.load("en_core_web_sm")

# Example text
text = """
Elon Musk, the CEO of SpaceX and Tesla, announced the launch of the Starship rocket from Boca Chica, Texas, 
on February 10, 2025. The launch is expected to revolutionize space travel.
"""

# Process the text using spaCy
doc = nlp(text)

# Extract named entities
print("Named Entities and Their Categories:")
for ent in doc.ents:
    print(f"{ent.text} ({ent.label_})")

# Explanation of entities
print("\nEntity Labels and Their Meanings:")
print(spacy.explain("ORG"))  # Example
print(spacy.explain("DATE"))  # Example



^^^^pra9:-Implement Sentiment Analysis 

from nltk.sentiment import SentimentIntensityAnalyzer
import nltk

# Download the SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# Initialize the sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Example text
texts = [
    "I love this product! It's amazing and works perfectly.",
    "This is the worst experience I've ever had.",
    "The product is okay, but it could be better.",
]

# Analyze sentiment for each text
for text in texts:
    scores = sia.polarity_scores(text)
    print(f"Text: {text}")
    print(f"Sentiment Scores: {scores}")
    if scores['compound'] > 0:
        print("Overall Sentiment: Positive")
    elif scores['compound'] < 0:
        print("Overall Sentiment: Negative")
    else:
        print("Overall Sentiment: Neutral")
    print("-" * 50)


()($$pra10:-Implement One real time NLP Application on a dataset available

pip install wordcloud
import pandas as pd
from collections import Counter 
import matplotlib.pyplot as plt 
from wordcloud import WordCloud
# Load the CSV file (replace 'your_file.csv' with the actual 
filename)
df = pd.read_csv('C:/Users/mera/Downloads/TestLarge.csv')
# Ensure the required columns are present
if 'Review' not in df.columns or 'Sentiment' not in 
df.columns:
raise ValueError("Columns 'review' or 'sentiment' not 
found in the CSV. Please check your file.")
# --- 1. Sentiment Distribution ---
sentiment_counts = df['Sentiment'].value_counts()
47
print("Sentiment Distribution:") 
print(sentiment_counts)
# Plot Sentiment Distribution
sentiment_counts.plot(kind='bar', color=['green', 'red', 
'blue'])
plt.title('Sentiment Distribution') 
plt.xlabel('Sentiment') 
plt.ylabel('Count') 
plt.xticks(rotation=0)
plt.show()
# --- 2. Word Frequency Analysis by Sentiment ---
# Function to clean and tokenize text
def clean_text(text): 
text = text.lower() 
words = text.split() 
return words
# Separate reviews by sentiment
positive_reviews = df[df['Sentiment'] == 'Positive']['Review'] 
negative_reviews = df[df['Sentiment'] == 'Negative']['Review'] 
neutral_reviews = df[df['Sentiment'] == 'Neutral']['Review']
# Combine all reviews by sentiment
positive_words = sum(positive_reviews.apply(clean_text), []) 
negative_words = sum(negative_reviews.apply(clean_text), []) 
neutral_words = sum(neutral_reviews.apply(clean_text), [])
# Get word frequency counts 
positive_word_counts = Counter(positive_words)
48
negative_word_counts = Counter(negative_words) 
neutral_word_counts = Counter(neutral_words)
# Display most common words for each sentiment 
print("\nMost Common Words in Positive Sentiments:") 
print(positive_word_counts.most_common(10))
print("\nMost Common Words in Negative Sentiments:") 
print(negative_word_counts.most_common(10))
print("\nMost Common Words in Neutral Sentiments:") 
print(neutral_word_counts.most_common(10))
# --- 3. Word Cloud for Positive Sentiment ---
positive_wordcloud = WordCloud(width=800, height=400, 
background_color='white').generate_from_frequencies(positive_w 
ord_counts)
plt.figure(figsize=(8, 8)) 
plt.imshow(positive_wordcloud, interpolation='bilinear') 
plt.title('Word Cloud for Positive Sentiment') 
plt.axis('off')
plt.show()
# --- 4. Word Cloud for Negative Sentiment ---
negative_wordcloud = WordCloud(width=800, height=400, 
background_color='white').generate_from_frequencies(negative_w 
ord_counts)
plt.figure(figsize=(8, 8)) 
plt.imshow(negative_wordcloud, interpolation='bilinear') 
plt.title('Word Cloud for Negative Sentiment') 
plt.axis('off')
plt.show()
49
# --- 5. Optional: Sentiment Over Time (if a timestamp column 
exists) ---
if 'timestamp' in df.columns:
# Convert timestamp to datetime
df['timestamp'] = pd.to_datetime(df['timestamp'])
# Group by sentiment and timestamp 
sentiment_time_series =
df.groupby([df['timestamp'].dt.to_period('M'),
'Sentiment']).size().unstack().fillna(0)
sentiment_time_series.plot(kind='line', marker='o') 
plt.title('Sentiment Over Time')
plt.xlabel('Time (Month-Year)') 
plt.ylabel('Count') 
plt.xticks(rotation=45) 
plt.show()
else:
print("No timestamp column found for sentiment over time 
analysis.")




